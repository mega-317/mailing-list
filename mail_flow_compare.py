import os
import json
from datetime import datetime
from statistics import mean, median

version = 1
label_dir = './data/seworld_label'
predict_dir = f'./prediction/validate'

fields_to_compare = [
    "conference_name",
    "start_date",
    "submission_deadline",
    "conference_website"
]

def parse_date(s):
    if not s:
        return None
    try:
        # ISO YYYY-MM-DD ê°€ì •
        return datetime.fromisoformat(s).date()
    except Exception:
        return None

# ì§‘ê³„ìš©
match_count = 0
comparison_results = []

# í•„ë“œë³„ ì§‘ê³„
per_field = {
    # ì´ì§„ ë¶„ë¥˜ ì „ìš© (ë¼ë²¨ vs ì˜ˆì¸¡)
    "is_call_for_paper": {
        "total": 0, "matches": 0, "errors": 0,
        "tp": 0, "fp": 0, "tn": 0, "fn": 0
    }
}
for f in fields_to_compare:
    per_field[f] = {
        "total": 0, "matches": 0, "errors": 0,
        "null_to_value": 0, "value_to_null": 0, "wrong_value": 0,
        # ë‚ ì§œìš© ì§€í‘œ
        "abs_day_errors": []  # ë‘˜ ë‹¤ ê°’ì´ ìˆê³  ë¶ˆì¼ì¹˜ì¸ ê²½ìš° |days| ëˆ„ì 
    }

# ë¹„êµ ë£¨í”„
for i in range(1, 101):
    label_path = os.path.join(label_dir, f"{i}_label.json")
    predict_path = os.path.join(predict_dir, f"{i}.json")

    with open(label_path, 'r', encoding='utf-8') as f:
        label_data = json.load(f)
    with open(predict_path, 'r', encoding='utf-8') as f:
        predict_data = json.load(f)

    mismatched_fields = []

    # --- ì´ì§„: is_call_for_paper vs is_call_for_conference_paper ---
    y = label_data.get("is_call_for_paper")
    yhat = predict_data.get("is_valid_cfp")
    per_field["is_call_for_paper"]["total"] += 1
    if y == yhat:
        per_field["is_call_for_paper"]["matches"] += 1
    else:
        per_field["is_call_for_paper"]["errors"] += 1
        mismatched_fields.append({
            "field": "is_cfp",
            "label_value": y,
            "predict_value": yhat,
        })
        
    # í˜¼ë™í–‰ë ¬
    if y is True and yhat is True:
        per_field["is_call_for_paper"]["tp"] += 1
    elif y is False and yhat is True:
        per_field["is_call_for_paper"]["fp"] += 1
    elif y is False and yhat is False:
        per_field["is_call_for_paper"]["tn"] += 1
    elif y is True and yhat is False:
        per_field["is_call_for_paper"]["fn"] += 1
        

    # --- ì¼ë°˜ í•„ë“œ ---
    # for field in fields_to_compare:
    #     # label_value = label_data.get("conference_name")
    #     # predict_value = predict_data.get("infos", {}).get("conf_name_final")
        
    #     label_value = label_data.get(field)
    #     predict_value = predict_data.get(field)
    #     # predict_value = predict_data.get("infos")
    #     per_field[field]["total"] += 1

    #     if label_value == predict_value:
    #         per_field[field]["matches"] += 1
    #         continue

    #     # ë¶ˆì¼ì¹˜ ê¸°ë¡
    #     per_field[field]["errors"] += 1
    #     if label_value is None and predict_value is not None:
    #         per_field[field]["null_to_value"] += 1
    #     elif label_value is not None and predict_value is None:
    #         per_field[field]["value_to_null"] += 1
    #     else:
    #         per_field[field]["wrong_value"] += 1

    #     # ìƒì„¸ ëª©ë¡ì— ì¶”ê°€
    #     mismatched_fields.append({
    #         "field": field,
    #         "label_value": label_value,
    #         "predict_value": predict_value
    #     })

    if mismatched_fields:
        comparison_results.append({
            "file": f"{i}_predict.json",
            "mismatches": mismatched_fields
        })
    else:
        match_count += 1

# --- ìš”ì•½ ë©”íŠ¸ë¦­ ê³„ì‚° ---
def safe_div(n, d):
    return n / d if d else 0.0

# is_call_for_paper ë©”íŠ¸ë¦­
cm = per_field["is_call_for_paper"]
precision = safe_div(cm["tp"], (cm["tp"] + cm["fp"]))
recall = safe_div(cm["tp"], (cm["tp"] + cm["fn"]))
f1 = safe_div(2 * precision * recall, (precision + recall))
binary_accuracy = safe_div(cm["matches"], cm["total"])

# í•„ë“œë³„ ì •í™•ë„ ë° ë‚ ì§œ ì˜¤ì°¨ ìš”ì•½
field_summaries = {}
for f in fields_to_compare:
    data = per_field[f]
    acc = safe_div(data["matches"], data["total"])
    mae = mean(data["abs_day_errors"]) if data["abs_day_errors"] else None
    medae = median(data["abs_day_errors"]) if data["abs_day_errors"] else None
    field_summaries[f] = {
        "matches": data["matches"],
        "errors": data["errors"],
        "null_to_value": data["null_to_value"],
        "value_to_null": data["value_to_null"],
        "wrong_value": data["wrong_value"],
    }

summary = {
    "overall": {
        "total_files": 100,
        "files_all_correct": match_count,
        "files_with_any_error": len(comparison_results),
    },
    "per_field": {
        "is_call_for_paper": {
            "matches": cm["matches"],
            "errors": cm["errors"],
            "label_true_predict_false": cm["fn"],  # ì‹¤ì œ True, ì˜ˆì¸¡ False
            "label_false_predict_true": cm["fp"],  # ì‹¤ì œ False, ì˜ˆì¸¡ True
        },
        **field_summaries
    }
}

# --- ì¶œë ¥ ë° ì €ì¥ ---
print(f"ì´ {match_count}ê°œì˜ íŒŒì¼ì´ ì „ë¶€ ì¼ì¹˜í•©ë‹ˆë‹¤.")
for item in comparison_results:
    print(f"\nâŒ {item['file']} ë¶ˆì¼ì¹˜:")
    for mismatch in item['mismatches']:
        print(f"  - í•„ë“œ: {mismatch['field']}")
        print(f"    ë¼ë²¨ ê°’: {mismatch['label_value']}")
        print(f"    ì˜ˆì¸¡ ê°’: {mismatch['predict_value']}")

out_path = f"result/comparison_result_{version}.json"
with open(out_path, "w", encoding="utf-8") as out_file:
    json.dump({
        "summary": summary,
        "match_count": match_count,
        "total_files": 100,
        "mismatched_files": comparison_results
    }, out_file, ensure_ascii=False, indent=2)

print(f"\nğŸ“ ê²°ê³¼ê°€ '{out_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
